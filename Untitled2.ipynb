{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbaca669",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'D' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27392/2073591725.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#     print(type(value[1][0]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'2020/4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mcontents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'D' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "content=pd.read_csv(r'C:\\Users\\dongxs\\Desktop\\luoyunting\\result_data_20210824\\weibo_weibos.csv')\n",
    "time=content['publish_time']\n",
    "data=content['content']\n",
    "contents=[]\n",
    "for value in enumerate(zip(time,data)):\n",
    "#     print(value)\n",
    "#     print(type(value[1][0]))\n",
    "    if isinstance(value[1][0],str) and value[1][0].startswith('2020/4'):\n",
    "        contents.append(D)\n",
    "        \n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebbf9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer,BertConfig,BertModel\n",
    "\n",
    "model_name='bert-base-cased'\n",
    "MODEL_PATH='I:/bert-base-cased/'\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained(model_name)\n",
    "model_config=BertConfig.from_pretrained(model_name)\n",
    "\n",
    "model_config.output_hidden_states=True\n",
    "model_config.output_attentions=True\n",
    "\n",
    "bert_model=BertModel.from_pretrained(MODEL_PATH,config=model_config)\n",
    "\n",
    "print(tokenizer.encode(\"I don't like you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f2db558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O\n",
      "B B\n",
      "I I\n",
      "I O\n",
      "O O\n",
      "B B\n",
      "O O\n",
      "B B\n",
      "I I\n",
      "I B\n",
      "O O\n",
      "O O\n",
      "O O\n",
      "11\n",
      "13 13\n",
      "0.8461538461538461\n"
     ]
    }
   ],
   "source": [
    "def accuracy(list1,list2):\n",
    "    count=0\n",
    "    for predict,orginal in  zip(list1,list2):\n",
    "        print(predict,orginal)\n",
    "        if predict==orginal:\n",
    "            count += 1\n",
    "    print(count)\n",
    "    print(len(list1),len(list2))\n",
    "    return count/len(list1)\n",
    "\n",
    "list1=['O','B','I','I','O','B','O','B','I','I','O','O','O']\n",
    "list2=['O','B','I','O','O','B','O','B','I','B','O','O','O']\n",
    "\n",
    "print(accuracy(list1,list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d53cde2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7571, 0.6636, 0.0417, 0.1518, 0.5329, 0.8763, 0.6031, 0.4046, 0.7802,\n",
      "         0.9127],\n",
      "        [0.2400, 0.6901, 0.9575, 0.9581, 0.5836, 0.7711, 0.2220, 0.5467, 0.2032,\n",
      "         0.7275]])\n",
      "tensor([[[-0.0554,  0.1702, -0.0715, -0.0925,  0.1722, -0.1536, -0.0122,\n",
      "           0.0569,  0.0311, -0.0153]],\n",
      "\n",
      "        [[-0.0557,  0.1704, -0.0716, -0.0930,  0.1714, -0.1529, -0.0116,\n",
      "           0.0574,  0.0311, -0.0158]]], grad_fn=<AddBackward0>)\n",
      "------------------------------------------\n",
      "tensor([[[0.4978, 0.5022],\n",
      "         [0.4987, 0.5013]]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "a=torch.rand(2,10)\n",
    "print(a)\n",
    "linear=nn.Linear(10,10)\n",
    "query=linear(a).unsqueeze(1)\n",
    "key=linear(a).unsqueeze(1)\n",
    "value=linear(a).unsqueeze(1)\n",
    "\n",
    "multihead_attn=nn.MultiheadAttention(embed_dim=10,num_heads=2)\n",
    "attn_ouput,attn_output_weights=multihead_attn(query,key,value)\n",
    "\n",
    "print(attn_ouput)\n",
    "print('------------------------------------------')\n",
    "print(attn_output_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a123edef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unable to parse I:/bert-base-cased/vocab.txt as a URL or as a local path",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3824/3534554491.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 加载词典 pre-trained model tokenizer (vocabulary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'I:/bert-base-cased/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Tokenized input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh:\\python3.7\\scripts\\lib\\site-packages\\pytorch_pretrained_bert\\tokenization.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;31m# redirect to the cache, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[0mresolved_vocab_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             logger.error(\n",
      "\u001b[1;32mh:\\python3.7\\scripts\\lib\\site-packages\\pytorch_pretrained_bert\\file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Something unknown\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unable to parse {} as a URL or as a local path\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: unable to parse I:/bert-base-cased/vocab.txt as a URL or as a local path"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    " \n",
    "# 加载词典 pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('I:/bert-base-cased/')\n",
    " \n",
    "# Tokenized input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    " \n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
    " \n",
    "# 将 token 转为 vocabulary 索引\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# 定义句子 A、B 索引\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    " \n",
    "# 将 inputs 转为 PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# 加载模型 pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    " \n",
    "# GPU & put everything on cuda\n",
    "tokens_tensor = tokens_tensor\n",
    "segments_tensors = segments_tensors\n",
    " \n",
    "# 得到每一层的 hidden states \n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "# 模型 bert-base-uncased 有12层，所以 hidden states 也有12层\n",
    "assert len(encoded_layers) == 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2227bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "spa_nlp=spacy.load('en_core_web_md')\n",
    "sent='Multiple cross-site scripting (XSS) vulnerabilities in yupdates_application.php in the Yahoo! Updates for WordPress plugin 1.0 and earlier for WordPress allow remote attackers to inject arbitrary web script or HTML via the (1) secret, (2) key, or (3) appid parameter.'\n",
    "sent1='Apache CouchDB 1.5.0 and earlier allows remote attackers to cause a denial of service (CPU and memory consumption) via the count parameter to /_uuids.'\n",
    "sent2='Openshift has shell command injection flaws due to unsanitized data being passed into shell commands.'\n",
    "doc=spa_nlp(sent1)\n",
    "tokens=[token for token in doc]\n",
    "displacy.serve(doc,style='dep')\n",
    "for token in doc:\n",
    "    #print(token.text,'-->head:  ',token.head.text,'  ',token.head.pos_,'  ',tokens.index(token.head))\n",
    "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
